package main

import (
	"archive/tar"
	"bufio"
	"compress/gzip"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"os"
	paths "path"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/briandowns/spinner"
	"github.com/colinmarc/hdfs"
	"github.com/dustin/go-humanize"
	"github.com/pkg/errors"
	"github.com/spf13/cobra"
	"golang.org/x/net/html"
	"gopkg.in/cheggaaa/pb.v1"
)

const (
	defaultGhtorrentMySQL = "http://ghtorrent-downloads.ewi.tudelft.nl/mysql/"
	defaultPGA            = "http://pga.sourced.tech/"
)

func fail(operation string, err error) {
	fmt.Fprintf(os.Stderr, "Error: %s: %s\n", operation, err.Error())
	os.Exit(1)
}

type discoveryParameters struct {
	URL           string
	StarsPath     string
	LanguagesPath string
	ReposPath     string
}

type trackingReader struct {
	RealReader io.Reader
	Callback   func(n int)
}

func (reader trackingReader) Read(p []byte) (n int, err error) {
	n, err = reader.RealReader.Read(p)
	reader.Callback(n)
	return n, err
}

func reduceWatchers(stream io.Reader) map[int]int {
	stars := map[int]int{}
	scanner := bufio.NewScanner(stream)
	for scanner.Scan() {
		line := scanner.Text()
		commaPos := strings.Index(line, ",")
		projectID, err := strconv.Atoi(line[:commaPos])
		if err != nil {
			fail(fmt.Sprintf("parsing watchers project ID \"%s\"", line[:commaPos]), err)
		}
		stars[projectID]++
	}
	return stars
}

type watchPair struct {
	Value int
	Key   int
}

func writeWatchers(stars map[int]int, ids map[int][]int, path string) {
	pairs := make([]watchPair, 0, len(stars))
	majors := map[int]bool{}
	for key := range stars {
		myids := ids[key]
		// if it is a reference, go to the main repository
		if myids[0] >= 0 {
			key = myids[0]
			myids = ids[key]
		}
		if majors[key] {
			continue
		}
		majors[key] = true
		maxval := stars[key]
		for _, id := range myids[1:] {
			other := stars[id]
			if other > maxval {
				maxval = other
			}
		}
		pairs = append(pairs, watchPair{Key: key, Value: maxval})
	}
	sort.Slice(pairs, func(i, j int) bool {
		return pairs[i].Value > pairs[j].Value // reverse order
	})
	f, err := os.Create(path)
	defer f.Close()
	if err != nil {
		fail("creating watchers file "+path, err)
	}
	for _, pair := range pairs {
		fmt.Fprintf(f, "%d %d\n", pair.Key, pair.Value)
	}
}

func writeProjects(stream io.Reader, path string) map[int][]int {
	f, err := os.Create(path)
	defer f.Close()
	if err != nil {
		fail("creating repositories file "+path, err)
	}
	gzf := gzip.NewWriter(f)
	defer gzf.Close()
	scanner := bufio.NewScanner(stream)
	skip := false
	projects := map[string]int{}
	ids := map[int][]int{}
	for scanner.Scan() {
		line := scanner.Text()
		skipThis := skip
		skip = line[len(line)-1:] == "\\"
		if skipThis {
			continue
		}
		commaPos := strings.Index(line, ",")
		if commaPos < 0 {
			fail("parsing projects "+line, fmt.Errorf("comma not found"))
		}
		projectID, err := strconv.Atoi(line[:commaPos])
		if err != nil {
			fail(fmt.Sprintf("parsing projects project ID \"%s\"", line[:commaPos]), err)
		}
		if projectID < 0 {
			continue
		}
		line = line[commaPos+1+30:] // +"https://api.github.com/repos/
		commaPos = strings.Index(line, "\"")
		projectName := line[:commaPos]
		// there can be duplicates
		// the slice is always at least one element
		// negative value means the original (main) repository
		// otherwise it is a reference to the main one
		if existingID, exists := projects[projectName]; !exists {
			projects[projectName] = projectID
			ids[projectID] = make([]int, 1, 1)
			ids[projectID][0] = -1
			fmt.Fprintf(gzf, "%d %s\n", projectID, projectName)
		} else {
			ids[existingID] = append(ids[existingID], projectID)
			ids[projectID] = make([]int, 1, 1)
			ids[projectID][0] = existingID
		}
	}
	return ids
}

func writeLanguages(stream io.Reader, path string) {
	f, err := os.Create(path)
	defer f.Close()
	if err != nil {
		fail("creating languages file "+path, err)
	}
	gzf := gzip.NewWriter(f)
	defer gzf.Close()
	scanner := bufio.NewScanner(stream)
	langs := map[string]map[int]bool{}
	for scanner.Scan() {
		line := scanner.Text()
		fields := strings.SplitN(line, ",", 3)
		projectID, err := strconv.Atoi(fields[0])
		if err != nil {
			fail("parsing project_languages.csv: "+line, err)
		}
		lang := fields[1][1 : len(fields[1])-1]
		projects := langs[lang]
		if projects == nil {
			projects = map[int]bool{}
			langs[lang] = projects
		}
		projects[projectID] = true
	}
	langList := make([]string, len(langs))
	{
		i := 0
		for lang := range langs {
			langList[i] = lang
			i++
		}
	}
	sort.Strings(langList)
	for _, lang := range langList {
		projects := langs[lang]
		projectsList := make([]int, len(projects))
		{
			i := 0
			for id := range projects {
				projectsList[i] = id
				i++
			}
		}
		sort.Ints(projectsList)
		fmt.Fprintf(gzf, "# %s\n", lang)
		for _, id := range projectsList {
			fmt.Fprintln(gzf, id)
		}
	}
}

func findMostRecentMySQLDump(root string) string {
	ghturl, err := url.Parse(root)
	if err != nil {
		fail("parsing "+root, err)
	}
	response, err := http.Get(ghturl.String())
	if err != nil {
		fail("connecting to "+ghturl.String(), err)
	}
	defer response.Body.Close()
	tokenizer := html.NewTokenizer(response.Body)
	dumps := []string{}
	for token := tokenizer.Next(); token != html.ErrorToken; token = tokenizer.Next() {
		if token == html.StartTagToken {
			tag := tokenizer.Token()
			if tag.Data == "a" {
				for _, attr := range tag.Attr {
					if attr.Key == "href" {
						dumps = append(dumps, attr.Val)
						break
					}
				}
			}
		}
	}
	if len(dumps) == 0 {
		fail("getting the list of available dumps", errors.New("no dumps found"))
	}
	sort.Strings(dumps)
	lastDumpStr := dumps[len(dumps)-1]
	dumpurl, err := url.Parse(lastDumpStr)
	if err != nil {
		fail("parsing "+lastDumpStr, err)
	}
	return ghturl.ResolveReference(dumpurl).String()
}

func discoverRepos(params discoveryParameters) {
	startTime := time.Now()
	spin := spinner.New(spinner.CharSets[11], 100*time.Millisecond)
	spin.Start()
	defer spin.Stop()
	var inputFile io.Reader
	if params.URL == "-" {
		inputFile = os.Stdin
	} else {
		if params.URL == "" {
			envURL := os.Getenv("GHTORRENT_MYSQL")
			if envURL == "" {
				envURL = defaultGhtorrentMySQL
			}
			spin.Suffix = " " + envURL
			params.URL = findMostRecentMySQLDump(envURL)
			fmt.Printf("\r>> %s\n", params.URL)
			spin.Suffix = " connecting..."
		}
		response, err := http.Get(params.URL)
		if err != nil {
			fail("starting the download of "+params.URL, err)
		}
		inputFile = response.Body
		defer response.Body.Close()
	}
	var totalRead int64
	inputFile = trackingReader{RealReader: inputFile, Callback: func(n int) {
		totalRead += int64(n)
		if totalRead%3 == 0 {
			// supposing that mod 3 is random, this is a quick and dirty way to (/ 3) updates.
			spin.Suffix = fmt.Sprintf(" %s", humanize.Bytes(uint64(totalRead)))
		}
	}}
	gzf, err := gzip.NewReader(inputFile)
	if err != nil {
		fail("opening gzip stream", err)
	}
	defer gzf.Close()
	tarf := tar.NewReader(gzf)
	processed := int64(0)
	numTasks := 3
	if params.LanguagesPath == "" {
		numTasks--
	}
	status := 0
	i := 0
	var stars map[int]int
	var ids map[int][]int
	for header, err := tarf.Next(); err != io.EOF; header, err = tarf.Next() {
		if err != nil {
			fail("reading tar.gz", err)
		}
		i++
		processed += header.Size
		isWatchers := strings.HasSuffix(header.Name, "watchers.csv")
		isProjects := strings.HasSuffix(header.Name, "projects.csv")
		isLanguages := strings.HasSuffix(header.Name, "project_languages.csv")
		mark := " "
		if isWatchers || isProjects || isLanguages {
			mark = ">"
		}
		strSize := humanize.Bytes(uint64(header.Size))
		if strings.HasSuffix(strSize, " B") {
			strSize += " "
		}
		if i == 1 {
			fmt.Print("\r", strings.Repeat(" ", 80))
		}
		fmt.Printf("\r%s %2d  %7s  %s\n", mark, i, strSize, header.Name)
		if isWatchers {
			stars = reduceWatchers(tarf)
			status++
		} else if isProjects {
			ids = writeProjects(tarf, params.ReposPath)
			status++
		} else if isLanguages && params.LanguagesPath != "" {
			writeLanguages(tarf, params.LanguagesPath)
			status++
		}
		if stars != nil && ids != nil {
			writeWatchers(stars, ids, params.StarsPath)
		}
		if status == numTasks {
			break
		}
	}
	fmt.Printf("\nRead      %s\nProcessed %s\nElapsed   %s\n",
		humanize.Bytes(uint64(totalRead)), humanize.Bytes(uint64(processed)), time.Since(startTime))
}

type selectionParameters struct {
	StarsFile         string
	LanguagesFile     string
	ReposFile         string
	FilteredLanguages []string
	MinStars          int
	TopN              int
	URLTemplate       string
}

func filterStars(path string, minStars int, topN int, selectedRepos map[int]bool) map[int]bool {
	f, err := os.Open(path)
	if err != nil {
		fail("opening stars file "+path, err)
	}
	defer f.Close()
	scanner := bufio.NewScanner(f)
	repos := map[int]bool{}
	var stars int
	for scanner.Scan() {
		if len(repos) >= topN && topN > -1 {
			fmt.Fprintf(os.Stderr, "\rEffective â˜… : %d%s\n", stars, strings.Repeat(" ", 40))
			break
		}
		line := scanner.Text()
		var repo int
		n, err := fmt.Sscan(line, &repo, &stars)
		if err != nil || n != 2 {
			if err == nil {
				err = errors.New("failed to parse " + line)
			}
			fail("parsing stars file "+path, err)
		}
		if selectedRepos != nil && !selectedRepos[repo] {
			continue
		}
		if stars >= minStars {
			repos[repo] = true
		} else {
			// the file is sorted
			break
		}
	}
	return repos
}

func filterLanguages(path string, languages []string) map[int]bool {
	gzf, err := os.Open(path)
	if err != nil {
		fail("opening languages file "+path, err)
	}
	defer gzf.Close()
	f, err := gzip.NewReader(gzf)
	if err != nil {
		fail("decompressing languages file "+path, err)
	}
	defer f.Close()
	scanner := bufio.NewScanner(f)
	langMap := map[string]bool{}
	for _, lang := range languages {
		langMap[lang] = true
	}
	result := map[int]bool{}
	active := false
	for scanner.Scan() {
		line := scanner.Text()
		if line[0] == '#' {
			active = langMap[line[2:]]
			continue
		}
		if !active {
			continue
		}
		id, err := strconv.Atoi(line)
		if err != nil {
			fail("parsing languages file "+path+": "+line, err)
		}
		result[id] = true
	}
	return result
}

func selectRepos(parameters selectionParameters) {
	spin := spinner.New(spinner.CharSets[11], 100*time.Millisecond)
	spin.Writer = os.Stderr
	spin.Start()

	var selectedRepos map[int]bool
	if len(parameters.FilteredLanguages) > 0 {
		spin.Suffix = " reading " + parameters.LanguagesFile
		selectedRepos = filterLanguages(parameters.LanguagesFile, parameters.FilteredLanguages)
	}
	spin.Suffix = " reading " + parameters.StarsFile
	selectedRepos = filterStars(
		parameters.StarsFile, parameters.MinStars, parameters.TopN, selectedRepos)
	spin.Stop()
	bar := pb.New(len(selectedRepos))
	bar.Output = os.Stderr
	bar.ShowFinalTime = true
	bar.ShowPercent = false
	bar.ShowSpeed = false
	bar.SetMaxWidth(80)
	bar.Start()
	defer bar.Finish()
	f, err := os.Open(parameters.ReposFile)
	if err != nil {
		fail("opening repositories file "+parameters.ReposFile, err)
	}
	defer f.Close()
	gzf, err := gzip.NewReader(f)
	if err != nil {
		fail("decompressing repositories file "+parameters.ReposFile, err)
	}
	defer gzf.Close()
	scanner := bufio.NewScanner(gzf)
	for scanner.Scan() {
		var repoID int
		var repoName string
		line := scanner.Text()
		n, err := fmt.Sscan(line, &repoID, &repoName)
		if err != nil || n != 2 {
			if err == nil {
				err = errors.New("failed to parse " + line)
			}
			fail("parsing repositories file "+parameters.ReposFile, err)
		}
		if selectedRepos[repoID] {
			bar.Increment()
			fmt.Fprintf(os.Stdout, parameters.URLTemplate+"\n", repoName)
		}
	}
}

func downloadIndex(baseURL string, outputPath string) {
	spin := spinner.New(spinner.CharSets[11], 100*time.Millisecond)
	spin.Writer = os.Stderr
	spin.Start()
	defer spin.Stop()
	indexURL, err := url.Parse(baseURL)
	if err != nil {
		fail("parsing "+baseURL, err)
	}
	latestURL, _ := url.Parse("csv/latest.csv.gz")
	effectiveURL := indexURL.ResolveReference(latestURL).String()
	response, err := http.Get(effectiveURL)
	if err != nil {
		fail("connecting to "+effectiveURL, err)
	}
	defer response.Body.Close()
	var totalRead int64
	inputFile := trackingReader{RealReader: response.Body, Callback: func(n int) {
		totalRead += int64(n)
		if totalRead%3 == 0 {
			// supposing that mod 3 is random, this is a quick and dirty way to (/ 3) updates.
			spin.Suffix = fmt.Sprintf(" %s", humanize.Bytes(uint64(totalRead)))
		}
	}}
	gzf, err := gzip.NewReader(inputFile)
	if err != nil {
		fail("opening gzip stream", err)
	}
	defer gzf.Close()
	var outputFile *os.File
	if outputPath == "-" {
		outputFile = os.Stdout
	} else {
		outputFile, err = os.Create(outputPath)
		if err != nil {
			fail("creating "+outputPath, err)
		}
	}
	bytesWritten, err := io.Copy(outputFile, gzf)
	spin.Stop()
	if err != nil {
		fail("failed to download "+effectiveURL, err)
	}
	fmt.Fprintf(os.Stderr, "Read      %s\nWrote     %s\n",
		humanize.Bytes(uint64(totalRead)), humanize.Bytes(uint64(bytesWritten)))
}

type downloadBackend interface {
	NewState() interface{}
	Download(state interface{}, response *http.Response)
}

type downloadManager struct {
	urls      chan *url.URL
	workers   int
	waitGroup *sync.WaitGroup
	backend   downloadBackend
	callback  func()
}

func (manager *downloadManager) Wait() {
	for i := 0; i < manager.workers; i++ {
		manager.urls <- nil
	}
	manager.waitGroup.Wait()
}

func (manager *downloadManager) work() {
	state := manager.backend.NewState()
	for {
		job := <-manager.urls
		if job == nil {
			manager.waitGroup.Done()
			break
		}
		manager.callback()
		response, err := http.Get(job.String())
		if err != nil {
			fmt.Fprintf(os.Stderr, "Failed to connect to %s\n", job.String())
			continue
		}
		func() {
			defer response.Body.Close()
			manager.backend.Download(state, response)
		}()
	}
}

func newDownloadManager(
	urls chan *url.URL, workers int, backend downloadBackend, callback func()) *downloadManager {
	manager := &downloadManager{
		urls:      urls,
		workers:   workers,
		waitGroup: &sync.WaitGroup{},
		backend:   backend,
		callback:  callback,
	}
	manager.waitGroup.Add(workers)
	for i := 0; i < workers; i++ {
		go manager.work()
	}
	return manager
}

type fileSystemDownloadBackend struct {
	OutputPath string
}

func (backend *fileSystemDownloadBackend) NewState() interface{} {
	return nil
}

func (backend *fileSystemDownloadBackend) Download(state interface{}, response *http.Response) {
	job := response.Request.URL
	length := response.ContentLength
	filePath := paths.Join(backend.OutputPath, job.Path)
	file, err := os.Open(filePath)
	if err == nil {
		fi, err := file.Stat()
		if err == nil {
			if fi.Size() == length {
				return
			}
		}
	}
	err = os.MkdirAll(paths.Dir(filePath), os.ModePerm)
	if err != nil {
		fail("failed to create directories: "+paths.Dir(filePath), err)
	}
	file, err = os.Create(filePath)
	if err != nil {
		fail("failed to create "+filePath, err)
	}
	written, err := io.Copy(file, response.Body)
	if err != nil {
		fail("failed to download "+job.String(), err)
	}
	if written != length {
		fail("incomplete download: "+job.String(), fmt.Errorf("%d != %d", written, length))
	}
}

type hdfsDownloadBackend struct {
	Address    string
	OutputPath string
}

func (backend *hdfsDownloadBackend) NewState() interface{} {
	if strings.HasPrefix(backend.OutputPath, "hdfs://") {
		panic("OutputPath must be without hdfs://")
	}
	client, err := hdfs.New(backend.Address)
	if err != nil {
		fail("connecting to HDFS at "+backend.Address, err)
	}
	return client
}

func (backend *hdfsDownloadBackend) Download(state interface{}, response *http.Response) {
	job := response.Request.URL
	client := state.(*hdfs.Client)
	length := response.ContentLength
	filePath := paths.Join(backend.OutputPath, job.Path)
	fi, err := client.Stat(filePath)
	if err == nil {
		if fi.Size() == length {
			return
		}
	}
	err = client.MkdirAll(paths.Dir(filePath), os.ModePerm)
	if err != nil {
		fail("failed to create directories: "+paths.Dir(filePath), err)
	}
	_ = client.Remove(filePath)
	file, err := client.Create(filePath)
	if err != nil {
		fail("failed to create "+filePath, err)
	}
	defer file.Close()
	written, err := io.Copy(file, response.Body)
	if err != nil {
		fail("failed to download "+job.String(), err)
	}
	if written != length {
		fail("incomplete download: "+job.String(), fmt.Errorf("%d != %d", written, length))
	}
}

func downloadDataset(baseURL string, series string, outPath string, workers int, hdfs string) {
	base, err := url.Parse(baseURL)
	if err != nil {
		fail("parsing "+baseURL, err)
	}
	bar := pb.New(0)
	bar.ShowFinalTime = true
	bar.ShowPercent = false
	bar.ShowSpeed = false
	bar.SetMaxWidth(80)
	bar.Start()
	defer bar.Finish()
	totalUrls := 0
	finishedUrls := 0
	urls := make(chan *url.URL, 1 << 24)
	defer close(urls)
	var backend downloadBackend
	if strings.HasPrefix(outPath, "hdfs://") {
		backend = &hdfsDownloadBackend{
			Address:    hdfs,
			OutputPath: outPath[7:],
		}
	} else {
		backend = &fileSystemDownloadBackend{
			OutputPath: outPath,
		}
	}
	downloader := newDownloadManager(urls, workers, backend, func() {
		finishedUrls++
		bar.Set(finishedUrls)
	})
	scanner := bufio.NewScanner(os.Stdin)
	for scanner.Scan() {
		line := scanner.Text()
		fileURL, err := url.Parse(fmt.Sprintf("siva/%s/%s/%s", series, line[:2], line))
		if err != nil {
			fail("failed to parse file names: "+scanner.Text(), err)
		}
		totalUrls++
		bar.Total = int64(totalUrls)
		urls <- base.ResolveReference(fileURL)
	}
	downloader.Wait()
}

var rootCmd = &cobra.Command{
	Use:   "multitool",
	Short: "Tool to generate the list of GitHub repository URLs [src-d/borges] from a GHTorrent MySQL dump.",
	Long: `Tool to generate the list of GitHub repository URLs [src-d/borges] from a GHTorrent MySQL dump.
There are two stages:

- discover: fetch the GHTorrent MySQL dump and extract the full list of repositories.
- select: filter the full list by the specified criteria.`,
}

var discoverCmd = &cobra.Command{
	Use: "discover",
	Short: "Fetch the GHTorrent MySQL dump and extract the list of repositories and the stars " +
		"per repository.",
	Long: `TODO`,
	Args: cobra.MaximumNArgs(0),
	Run: func(cmd *cobra.Command, args []string) {
		flags := cmd.Flags()
		url, _ := flags.GetString("url")
		stars, _ := flags.GetString("stars")
		langs, _ := flags.GetString("languages")
		repos, _ := flags.GetString("repos")
		parameters := discoveryParameters{
			URL:           url,
			StarsPath:     stars,
			LanguagesPath: langs,
			ReposPath:     repos,
		}
		discoverRepos(parameters)
	},
}

var selectCmd = &cobra.Command{
	Use: "select",
	Short: "Reduce the full list of repositories from \"discover\" by the specified filters and " +
		"write the result to stdout.",
	Long: `TODO`,
	Args: cobra.MaximumNArgs(0),
	Run: func(cmd *cobra.Command, args []string) {
		flags := cmd.Flags()
		starsFile, _ := flags.GetString("stars")
		langs, _ := flags.GetString("languages")
		reposFile, _ := flags.GetString("repos")
		minStars, _ := flags.GetInt("min-stars")
		filteredLangs, _ := flags.GetString("filter-languages")
		topN, _ := flags.GetInt("max")
		urlTemplate, _ := flags.GetString("url-template")
		var filteredLangsSplitted []string
		if len(filteredLangs) == 0 {
			filteredLangsSplitted = []string{}
		} else {
			filteredLangsSplitted = strings.Split(filteredLangs, ",")
		}
		selectRepos(selectionParameters{
			StarsFile:         starsFile,
			LanguagesFile:     langs,
			ReposFile:         reposFile,
			MinStars:          minStars,
			FilteredLanguages: filteredLangsSplitted,
			TopN:              topN,
			URLTemplate:       urlTemplate,
		})
	},
}

var getIndexCmd = &cobra.Command{
	Use:   "get-index",
	Short: "Download the most recent dataset index file.",
	Long:  "TODO",
	Args:  cobra.MaximumNArgs(0),
	Run: func(cmd *cobra.Command, args []string) {
		flags := cmd.Flags()
		outPath, _ := flags.GetString("output")
		baseURL, _ := flags.GetString("base")
		downloadIndex(baseURL, outPath)
	},
}

var getDatasetCmd = &cobra.Command{
	Use:   "get-dataset",
	Short: "Download Siva files from the given list in stdin.",
	Long:  "TODO",
	Args:  cobra.MaximumNArgs(0),
	Run: func(cmd *cobra.Command, args []string) {
		flags := cmd.Flags()
		outPath, _ := flags.GetString("output")
		baseURL, _ := flags.GetString("base")
		series, _ := flags.GetString("series")
		workers, _ := flags.GetInt("workers")
		hdfs, _ := flags.GetString("hdfs")
		downloadDataset(baseURL, series, outPath, workers, hdfs)
	},
}

func init() {
	rootCmd.AddCommand(discoverCmd)
	rootCmd.AddCommand(selectCmd)
	rootCmd.AddCommand(getIndexCmd)
	rootCmd.AddCommand(getDatasetCmd)
	discoverFlags := discoverCmd.Flags()
	discoverFlags.StringP(
		"url", "l", "",
		"Link to GHTorrent MySQL dump in tar.gz format. If \"-\", stdin is read. If empty (default), "+
			"find the most recent dump at GHTORRENT_MYSQL ?= "+defaultGhtorrentMySQL+".")
	discoverFlags.StringP(
		"stars", "s", "",
		"Output path for the file with the numbers of stars per repository.")
	discoverFlags.StringP(
		"languages", "g", "",
		"Output path for the gzipped file with the mapping between languages and repositories. "+
			"May be empty - will be skipped then.")
	discoverFlags.StringP(
		"repos", "r", "",
		"Output path for the gzipped file with the repository names and identifiers.")
	discoverCmd.MarkFlagRequired("stars")
	discoverCmd.MarkFlagRequired("repos")

	selectFlags := selectCmd.Flags()
	selectFlags.StringP(
		"stars", "s", "",
		"Input path for the file with the numbers of stars per repository.")
	selectFlags.StringP(
		"languages", "g", "",
		"Input path for the gzipped file with the mapping between languages and repositories.")
	selectFlags.StringP(
		"repos", "r", "",
		"Input path for the gzipped file with the repository names and identifiers.")
	selectFlags.IntP("min-stars", "m", 0, "Minimum number of stars.")
	selectFlags.IntP("max", "n", -1,
		"Maximum number of top-starred repositories to clone. -1 means unlimited. Language filter "+
			"is applied before.")
	selectFlags.StringP("filter-languages", "l", "", "Comma separated list of languages.")
	selectFlags.String("url-template", "git://github.com/%s.git", "Output URL printf template.")
	selectCmd.MarkFlagRequired("repos")
	selectCmd.MarkFlagRequired("stars")

	getIndexFlags := getIndexCmd.Flags()
	getIndexFlags.StringP("base", "b", defaultPGA, "Base URL for index files.")
	getIndexFlags.StringP("output", "o", "-", "Output path. \"-\" means stdout (default).")

	getDatasetFlags := getDatasetCmd.Flags()
	getDatasetFlags.StringP("base", "b", defaultPGA, "Base URL for Siva files.")
	getDatasetFlags.StringP("series", "s", "latest", "Snapshot name.")
	getDatasetFlags.StringP("output", "o", "", "Output directory. Required. If starts with "+
		"\"hdfs://\", files are written to HDFS.")
	getDatasetFlags.String("hdfs", "", "HDFS namenode address:port.")
	getDatasetFlags.IntP("workers", "j", 10, "The number of parallel goroutines to run to "+
		"download files.")
}

func main() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		os.Exit(1)
	}
}
